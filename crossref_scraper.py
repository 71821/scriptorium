# ========================================
# üìå Description du script :
#
# Ce script : 
# - Recherche les publications li√©es √† un mot-cl√© depuis l'API Crossref.
# - Extrait les m√©tadonn√©es : titre, auteur, date, DOI, r√©sum√©
# - Les r√©sultats sont sauvegard√©s par tranches de 500 publications.
# - Met √† jour automatiquement un fichier global fusionn√© apr√®s chaque tranche.
# - Reprend automatiquement l√† o√π il s‚Äôest arr√™t√© en cas d‚Äôinterruption.
# - G√®re les erreurs et les connexions lentes (jusqu'√† 60 tentatives de 5s)
# - Fournit une progression en pourcentage dans la console.
# ========================================


import requests
import time
import os
import glob
import pandas as pd

# === FONCTION PRINCIPALE ===
def fetch_crossref_data(mot_cle):
    nom_dossier = f"resultats_{mot_cle.replace(' ', '_')}"
    os.makedirs(nom_dossier, exist_ok=True)
    fichier_cursor = os.path.join(nom_dossier, "cursor.txt")
    fichier_combine = f"{nom_dossier}.xlsx"
    email_contact = "votre.email@example.com"

    if os.path.exists(fichier_cursor):
        with open(fichier_cursor, "r") as f:
            cursor = f.read().strip()
        print("üîÅ Reprise √† partir du dernier curseur enregistr√©.")
    else:
        cursor = "*"
        print("üöÄ Nouvelle recherche commenc√©e.")

    try:
        r_init = requests.get(
            "https://api.crossref.org/works",
            params={
                "query.bibliographic": mot_cle,
                "rows": 0,
                "mailto": email_contact
            },
            timeout=30
        )
        r_init.raise_for_status()
        total = r_init.json()["message"]["total-results"]
        print(f"\U0001f4ca Nombre total estim√© de publications trouv√©es : {total}")
    except Exception as e:
        print(f"\u26a0\ufe0f Impossible d‚Äôobtenir le total initial : {e}")
        total = None

    def enregistrer_chunk(data, numero):
        df = pd.DataFrame(data)
        fichier = os.path.join(nom_dossier, f"chunk_{numero}.xlsx")
        df.to_excel(fichier, index=False)
        print(f"üìÇ Chunk {numero} sauvegard√© ({len(df)} lignes)")
        return fichier

    def maj_fichier_combine():
        fichiers = sorted(glob.glob(os.path.join(nom_dossier, "chunk_*.xlsx")))
        all_dfs = []
        for f in fichiers:
            try:
                df = pd.read_excel(f)
                all_dfs.append(df)
            except Exception as e:
                print(f"‚ùå Erreur lecture {f} : {e}")
        if all_dfs:
            df_final = pd.concat(all_dfs, ignore_index=True)
            df_final.to_excel(fichier_combine, index=False)
            print(f"üìÅ Fichier combin√© mis √† jour : {fichier_combine} ({len(df_final)} lignes)")

    def trouver_prochain_numero():
        existants = glob.glob(os.path.join(nom_dossier, "chunk_*.xlsx"))
        if not existants:
            return 1
        numeros = [int(os.path.basename(f).split("_")[1].split(".")[0]) for f in existants]
        return max(numeros) + 1

    chunk_num = trouver_prochain_numero()
    count_total = (chunk_num - 1) * 500

    while True:
        try:
            for tentative in range(60):
                try:
                    r = requests.get(
                        "https://api.crossref.org/works",
                        params={
                            "query.bibliographic": mot_cle,
                            "rows": 500,
                            "cursor": cursor,
                            "mailto": email_contact,
                            "select": "title,author,issued,DOI,URL,abstract,subject"
                        },
                        timeout=30
                    )
                    r.raise_for_status()
                    break
                except Exception as e:
                    print(f"‚ö†\ufe0f Erreur (tentative {tentative+1}/60) : {e}")
                    time.sleep(5)
            else:
                print("‚ùå Abandon apr√®s 60 tentatives.")
                break

            data = r.json()["message"]
            items = data["items"]
            if not items:
                print("‚úÖ Extraction termin√©e.")
                break

            lignes = []
            for item in items:
                lignes.append({
                    "titre": item.get("title", [""])[0],
                    "auteurs": "; ".join([f"{a.get('given', '')} {a.get('family', '')}" for a in item.get("author", [])]) if item.get("author") else "",
                    "date": "-".join(map(str, item.get("issued", {}).get("date-parts", [[None]])[0])),
                    "DOI": item.get("DOI", ""),
                    "URL": item.get("URL", ""),
                    "abstract": item.get("abstract", ""),
                    "mots_cles": "; ".join(item.get("subject", [])) if item.get("subject") else ""
                })

            fichier_chunk = enregistrer_chunk(lignes, chunk_num)
            count_total += len(lignes)
            maj_fichier_combine()

            if total:
                pourcentage = count_total / total * 100
                print(f"üìà Progression : {count_total}/{total} ({pourcentage:.2f}%)\n")
            else:
                print(f"üìà Progression : {count_total} lignes extraites\n")

            chunk_num += 1
            cursor = data["next-cursor"]
            with open(fichier_cursor, "w") as f:
                f.write(cursor)

            time.sleep(1 + 5 * (chunk_num % 3))

        except KeyboardInterrupt:
            print("‚èπÔ∏è Interruption par l‚Äôutilisateur.")
            break

# === POINT D‚ÄôENTR√âE DU SCRIPT ===
if __name__ == "__main__":
    print("""
-----------------------------------------------------
üìò Script Crossref ‚Äì Extraction massive de publications
-----------------------------------------------------
Ce script :
‚úîÔ∏è Recherche toutes les publications associ√©es √† un mot-cl√©
‚úîÔ∏è Extrait les m√©tadonn√©es : titre, auteur, date, DOI, r√©sum√©
‚úîÔ∏è Sauvegarde automatiquement par tranches de 500 r√©sultats
‚úîÔ∏è Met √† jour automatiquement un fichier combin√© global
‚úîÔ∏è G√®re les erreurs et les connexions lentes (jusqu‚Äô√† 60 essais)

‚ö†Ô∏è Important :
Le mot-cl√© sera recherch√© dans le **titre** et le champ **bibliographique**.
-----------------------------------------------------
""")
    keyword = input("üîé Entrez le mot-cl√© √† rechercher : ").strip()
    if not keyword:
        print("‚ùå Vous devez entrer un mot-cl√© valide.")
    else:
        fetch_crossref_data(keyword)
