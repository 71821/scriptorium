
# ========================================
# ğŸ“Œ Description du script :
#
# Ce script : 
# - Recherche les publications liÃ©es Ã  un mot-clÃ© depuis l'API Crossref.
# - Extrait les mÃ©tadonnÃ©es : titre, auteur, date, DOI, rÃ©sumÃ©
# - Les rÃ©sultats sont sauvegardÃ©s par tranches de 500 publications.
# - Met Ã  jour automatiquement un fichier global fusionnÃ© aprÃ¨s chaque tranche.
# - Demande s'il faut reprendre lÃ  oÃ¹ il sâ€™est arrÃªtÃ© ou non en cas dâ€™interruption.
# - GÃ¨re les erreurs et les connexions lentes (jusqu'Ã  60 tentatives de 5s)
# - Fournit une progression en pourcentage dans la console.
# ========================================


import requests
import time
import os
import json
import pandas as pd
from urllib.parse import quote

def fetch_crossref_data(keyword, resume=False):
    encoded_keyword = quote(keyword)
    base_url = "https://api.crossref.org/works"
    rows_per_request = 500
    email = "votre.email@example.com"
    headers = {"User-Agent": f"Python Script (mailto:{email})"}

    # Dossiers de sauvegarde
    output_folder = f"resultats_{keyword.replace(' ', '_')}"
    os.makedirs(output_folder, exist_ok=True)
    combined_file = f"{output_folder}.xlsx"
    cursor_file = os.path.join(output_folder, "cursor.txt")

    # Reprendre Ã  partir du dernier curseur ?
    if resume and os.path.exists(cursor_file):
        with open(cursor_file, "r") as f:
            cursor = f.read().strip()
        print("ğŸ” Reprise intelligente Ã  partir du dernier curseur sauvegardÃ©...")
    else:
        cursor = "*"
        print("ğŸš€ Nouvelle extraction depuis le dÃ©but...")

    # Chercher le nombre total estimÃ© de rÃ©sultats
    try:
        count_response = requests.get(
            f"{base_url}?query.bibliographic={encoded_keyword}&rows=0&mailto={email}",
            headers=headers, timeout=30
        )
        count_response.raise_for_status()
        total_results = count_response.json()["message"]["total-results"]
        print(f"\nğŸ“Š Nombre total estimÃ© de publications trouvÃ©es : {total_results}\n")
    except Exception as e:
        print(f"âŒ Erreur lors de la rÃ©cupÃ©ration du nombre total de rÃ©sultats : {e}")
        total_results = None

    all_chunks = []
    chunk_number = 1
    total_saved = 0

    if resume:
        # Trouver le dernier chunk existant
        existing_chunks = [f for f in os.listdir(output_folder) if f.startswith("chunk_") and f.endswith(".xlsx")]
        if existing_chunks:
            chunk_numbers = [int(f.split("_")[1].split(".")[0]) for f in existing_chunks]
            chunk_number = max(chunk_numbers) + 1
            total_saved = (chunk_number - 1) * rows_per_request

    while True:
        params = {
            "query.bibliographic": keyword,
            "rows": rows_per_request,
            "cursor": cursor,
            "mailto": email,
        }

        success = False
        for attempt in range(60):
            try:
                response = requests.get(base_url, params=params, headers=headers, timeout=30)
                response.raise_for_status()
                data = response.json()
                success = True
                break
            except Exception as e:
                print(f"âš ï¸ Erreur (tentative {attempt+1}/60) : {e}")
                time.sleep(5)

        if not success:
            print("âŒ Ã‰chec aprÃ¨s 60 tentatives. Fin de l'extraction.")
            break

        items = data["message"]["items"]
        if not items:
            print("âœ… Aucune donnÃ©e supplÃ©mentaire. Extraction terminÃ©e.")
            break

        chunk_data = []
        for item in items:
            title = item.get("title", [""])[0]
            authors = ", ".join([f"{a.get('given', '')} {a.get('family', '')}".strip() for a in item.get("author", [])]) if "author" in item else ""
            date_parts = item.get("issued", {}).get("date-parts", [[None]])
            year = date_parts[0][0]
            doi = item.get("DOI", "")
            url = item.get("URL", "")
            abstract = item.get("abstract", "")
            chunk_data.append({
                "Titre": title,
                "Auteurs": authors,
                "AnnÃ©e": year,
                "DOI": doi,
                "URL": url,
                "RÃ©sumÃ©": abstract
            })

        df_chunk = pd.DataFrame(chunk_data)
        chunk_path = os.path.join(output_folder, f"chunk_{chunk_number}.xlsx")
        df_chunk.to_excel(chunk_path, index=False)
        print(f"\nğŸ’¾ Chunk {chunk_number} sauvegardÃ© ({len(df_chunk)} lignes)")

        # Mise Ã  jour fichier combinÃ©
        if os.path.exists(combined_file):
            df_existing = pd.read_excel(combined_file)
            df_combined = pd.concat([df_existing, df_chunk], ignore_index=True)
        else:
            df_combined = df_chunk
        df_combined.to_excel(combined_file, index=False)
        total_saved = len(df_combined)
        print(f"ğŸ“ Fichier combinÃ© mis Ã  jour : {combined_file} ({total_saved} lignes)")

        chunk_number += 1
        cursor = data["message"]["next-cursor"]
        with open(cursor_file, "w") as f:
            f.write(cursor)

        if total_results:
            percent = (total_saved / total_results) * 100
            print(f"ğŸ“ˆ Progression : {total_saved}/{total_results} ({percent:.2f}%)")
        else:
            print(f"ğŸ“ˆ Progression : {total_saved} lignes extraites...")

        # Pause alÃ©atoire entre 1 et 7 secondes
        time.sleep(1 + (6 * time.time() % 1))


if __name__ == "__main__":
    print("""
-----------------------------------------------------
ğŸ“˜ Script Crossref â€“ Extraction massive de publications
-----------------------------------------------------
Ce script :
âœ”ï¸ Recherche toutes les publications associÃ©es Ã  un mot-clÃ©
âœ”ï¸ Extrait les mÃ©tadonnÃ©es : titre, auteur, date, DOI, rÃ©sumÃ©
âœ”ï¸ Sauvegarde automatiquement par tranches de 500 rÃ©sultats
âœ”ï¸ Met Ã  jour automatiquement un fichier combinÃ© global
âœ”ï¸ GÃ¨re les erreurs et les connexions lentes (jusquâ€™Ã  60 essais)

âš ï¸ Important :
Le mot-clÃ© sera recherchÃ© dans le **titre** et le champ **bibliographique**.
-----------------------------------------------------
""")
    keyword = input("ğŸ” Entrez le mot-clÃ© Ã  rechercher : ").strip()
    if not keyword:
        print("âŒ Vous devez entrer un mot-clÃ© valide.")
    else:
        reprendre = input("ğŸ” Reprendre Ã  partir de la derniÃ¨re interruption ? (o/n) : ").strip().lower()
        fetch_crossref_data(keyword, resume=(reprendre == 'o'))
